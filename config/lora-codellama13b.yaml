base_model: codellama/CodeLlama-13b-Instruct-hf
hf_model_repo: anto0699/Prompt2OpenSCENARIO-CodeLlama13B-LoRA
hf_dataset_repo: anto0699/Prompt2OpenSCENARIO-LoRA-FineTune
output_dir: runs/codellama13b-lora
seed: 42

# QLoRA / PEFT
lora_r: 32
lora_alpha: 32
lora_dropout: 0.05
target_modules: [q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj]
bias: "none"

# Quantizzazione
load_in_4bit: false
#bnb_4bit_quant_type: "nf4"
#bnb_4bit_use_double_quant: true
#bnb_4bit_compute_dtype: "bfloat16"

# Training
max_seq_len: 3072
train_batch_size: 1
grad_accum_steps: 16
eval_batch_size: 1
learning_rate: 2.0e-4
weight_decay: 0.0
num_epochs: 2
warmup_ratio: 0.05
logging_steps: 10
eval_steps: 200
save_steps: 500
gradient_checkpointing: true
packing: true

# Generation (per validazione)
gen_max_new_tokens: 1800
gen_temperature: 0.3
gen_top_p: 0.9
stop_sequence: "</OpenSCENARIO>"

# Valutazione (slot & XSD)
xsd_path: ""   # inserisci il percorso a OpenSCENARIO.xsd se ce l'hai, altrimenti lascialo vuoto
val_split_ratio: 0.1
